{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "\n",
    "from ema_workbench import (\n",
    "    Model, Policy, Scenario, MultiprocessingEvaluator,\n",
    "    ema_logging\n",
    ")\n",
    "from ema_workbench.analysis import (\n",
    "    feature_scoring, parcoords, pairs_plotting, prim\n",
    ")\n",
    "from ema_workbench.em_framework.evaluators import (\n",
    "    BaseEvaluator, perform_experiments\n",
    ")\n",
    "from ema_workbench.em_framework.optimization import (\n",
    "    ArchiveLogger, EpsilonProgress, EpsNSGAII,\n",
    "    HyperVolume, epsilon_nondominated, to_problem\n",
    ")\n",
    "from ema_workbench.em_framework.outcomes import ScalarOutcome\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from ema_workbench.em_framework.samplers import sample_uncertainties\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench.analysis import dimensional_stacking\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "from ema_workbench import Model, Constraint\n",
    "from ema_workbench import RealParameter, CategoricalParameter, IntegerParameter, Model\n",
    "\n",
    "def get_do_nothing_dict():\n",
    "    return {l.name: 0 for l in model.levers}\n",
    "\n",
    "\n",
    "# Constraint function\n",
    "def max_one_rfr_total(*args):\n",
    "            total = float(np.sum(args))  # ensures scalar\n",
    "            violation = 0\n",
    "            for i in range(0, len(args), 3):\n",
    "                group_total = float(np.sum(args[i:i + 3]))\n",
    "                if group_total > 1:\n",
    "                    violation += group_total - 1\n",
    "            return violation\n",
    "\n",
    "# ---- RfR constraint: max 1 time step per project -----------------\n",
    "planning_steps=3\n",
    "rfr_constraints = []\n",
    "for pid in range(5):  # five RfR projects\n",
    "    lever_names = [f\"{pid}_RfR {t}\" for t in planning_steps]\n",
    "    rfr_lever_names = [f\"{area}_RfR {i}\" for area in range(5) for i in range(3)]\n",
    "            # rfr_constraints.append(\n",
    "            #     Constraint(\n",
    "            #         name=f\"RFR_once_proj{pid}\",\n",
    "            #         function=partial(at_most_one_rfr),\n",
    "            #         parameter_names=lever_names  # optional, for clarity\n",
    "            #     )\n",
    "\n",
    "            # Constraint object\n",
    "    rfr_constraints.append (Constraint(\n",
    "                \"max_one_rfr_total\",\n",
    "                function=max_one_rfr_total,\n",
    "                parameter_names=rfr_lever_names))\n",
    "\n"
   ],
   "id": "7cc928af6a0b60a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Start with running baseline scenario\n",
    "if __name__ == \"__main__\":\n",
    "    model, _ = get_model_for_problem_formulation(3)\n",
    "\n",
    "    with MultiprocessingEvaluator(model, n_processes=-1) as evaluator:\n",
    "        baseline = Policy(\"do_nothing\", **get_do_nothing_dict())\n",
    "        policies = [baseline]\n",
    "\n",
    "        results = evaluator.perform_experiments(scenarios=100, policies=policies)\n",
    "        baseline_experiments, baseline_outcomes = results\n"
   ],
   "id": "9fd0b2fb82b3a082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Start with running baseline scenario\n",
    "if __name__ == \"__main__\":\n",
    "    model, _ = get_model_for_problem_formulation(3)\n",
    "\n",
    "    with MultiprocessingEvaluator(model, n_processes=-1) as evaluator:\n",
    "\n",
    "        results = evaluator.perform_experiments(scenarios=100, policies=4)\n",
    "        experiments, outcomes = results"
   ],
   "id": "f4770d2d01f82b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ac99a17cc96c6610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "planning_steps=  range(3)\n",
    "proj_ids=  range(5)\n",
    "\n",
    "violations = pd.Series(False, index=baseline_experiments.index)   # start with all False\n",
    "\n",
    "for pid in proj_ids:\n",
    "    cols = [f\"{pid}_RfR {t}\" for t in planning_steps]    # the three levers\n",
    "    # True where a policy switches ‚â•2 time-steps on the same project\n",
    "    violations |= (baseline_experiments[cols].sum(axis=1) > 1)\n",
    "\n",
    "print(\"number of violating runs:\", violations.sum())\n",
    "print(\"runs that violate:\", baseline_experiments.index[violations].tolist()[:10], \"...\")"
   ],
   "id": "1f5f524b2a2dac61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running and showing the model",
   "id": "8f45449d05a654df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Load your model ---\n",
    "    model, planning_steps = get_model_for_problem_formulation(3)\n",
    "\n",
    "\n",
    "\n",
    "    reference_values = {\n",
    "        \"Bmax\": 175,\n",
    "        \"Brate\": 1.5,\n",
    "        \"pfail\": 0.5,\n",
    "        \"discount rate 0\": 3.5,\n",
    "        \"discount rate 1\": 3.5,\n",
    "        \"discount rate 2\": 3.5,\n",
    "        \"ID flood wave shape\": 4,\n",
    "    }\n",
    "    scen1 = {}\n",
    "\n",
    "    for key in model.uncertainties:\n",
    "        name_split = key.name.split(\"_\")\n",
    "\n",
    "        if len(name_split) == 1:\n",
    "            scen1.update({key.name: reference_values[key.name]})\n",
    "\n",
    "        else:\n",
    "            scen1.update({key.name: reference_values[name_split[1]]})\n",
    "\n",
    "    ref_scenario = Scenario(\"reference\", **scen1)\n",
    "\n",
    "\n",
    "    convergence_metrics = [\n",
    "\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "    espilon = [0.01] * len(model.outcomes)\n",
    "\n",
    "\n",
    "    nfe = 100\n",
    "\n",
    "    with MultiprocessingEvaluator(model) as evaluator:\n",
    "        results, convergence = evaluator.optimize(\n",
    "            nfe=nfe,\n",
    "            searchover=\"levers\",\n",
    "            epsilons=espilon,\n",
    "            convergence=convergence_metrics, reference=ref_scenario,\n",
    "            constraints = model.constraints,\n",
    "\n",
    "        )"
   ],
   "id": "dd9c3a53061f4150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Outcome-namen:\", [o.name for o in model.outcomes])\n",
    "print(\"Constraint-namen:\", [c.name for c in model.constraints])"
   ],
   "id": "6c9cdbd39ac5df9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6, 4))  # no ncols=2\n",
    "\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel(r'$\\epsilon$-progress')\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "plt.savefig(\"convergence enkel MORDM.png\", dpi=300, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "fb3772ddbb59e4f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = results.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['A.2 Total Costs', 'A.2_Expected Number of Deaths', 'RfR Total Costs',\"Expected Evacuation Costs\"]] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "    # Set larger figure size (wider and taller)\n",
    "plt.figure(figsize=(14, 10))  # width=12 inches, height=8 inches\n",
    "\n",
    "# Generate parallel axes with your limits\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "# Plot the data\n",
    "paraxes.plot(data)\n",
    "\n",
    "# Invert specific axis\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"mordm_parallel coordinates all.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "140c683e2f85f1f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logical = (\n",
    "    (results[\"A.2 Total Costs\"] < 2e8) &\n",
    "\n",
    "    (results[\"A.2_HRI per dike\"] > 0.4))\n",
    "\n",
    "\n",
    "\n",
    "np.sum(logical)"
   ],
   "id": "dd8a20bda91aeb4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_1 = results[logical]\n",
    "results_1[\"policy\"] = results_1.index  # Automatically uses 16, 17, 18 in your case\n",
    "\n",
    "\n",
    "data = results_1.loc[:, [o.name for o in model.outcomes] + ['policy']]\n",
    "\n",
    "\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['A.2 Total Costs', 'A.2_Expected Number of Deaths',\n",
    "               'RfR Total Costs', 'Expected Evacuation Costs']] = 0\n",
    "\n",
    "\n",
    "policy_ids = data[\"policy\"].unique()\n",
    "colors = sns.color_palette(\"tab10\", len(policy_ids))\n",
    "color_map = dict(zip(policy_ids, colors))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "# Plot each policy row with its assigned color\n",
    "for _, row in data.iterrows():\n",
    "    policy_id = row[\"policy\"]\n",
    "    color = color_map.get(policy_id, \"gray\")\n",
    "    paraxes.plot(row.to_frame().T, color=color, alpha=0.8)\n",
    "\n",
    "# Invert axis if needed\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], color=color_map[pid], label=f\"Policy {pid}\")\n",
    "    for pid in policy_ids\n",
    "]\n",
    "plt.legend(handles=legend_handles, title=\"Policy ID\", loc=\"center left\",bbox_to_anchor=(1.02, 0.5),borderaxespad=0)\n",
    "\n",
    "\n",
    "plt.savefig(\"parallel_coords_MORDM_selected_policies.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "8d98738f22e57cd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "833918448a2743da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a9b14464b622766b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb047c377b8d4ad2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## optimized policies",
   "id": "1e83538860186db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "policies = results[logical]\n",
    "policies = policies.drop([o.name for o in model.outcomes], axis=1)\n",
    "policies"
   ],
   "id": "cce0db644d5ad4e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))"
   ],
   "id": "143ca5fdd9a4289c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Create policy dataframe\n",
    "lever_names = [l.name for l in model.levers]\n",
    "#policies_df = results[lever_names]\n",
    "policies.to_csv(\"candidate_policies.csv\", index=False)"
   ],
   "id": "e69361bb53203803"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing out the policies",
   "id": "72bfe0a901780ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 100\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)"
   ],
   "id": "23158ae182772006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    if std==0:\n",
    "        std = 1\n",
    "\n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std\n"
   ],
   "id": "9abcc70f079ed1c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "\n",
    "    logical = experiments['policy']==policy\n",
    "\n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ],
   "id": "5277f984f2155129"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['A.2 Total Costs', 'A.2_Expected Number of Deaths', 'RfR Total Costs',\"Expected Evacuation Costs\"]] = 0\n",
    "\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.savefig(\"scores_single_MORDM.png\", dpi=300, bbox_inches='tight') # width=12 inches, height=8 inches\n",
    "plt.show()"
   ],
   "id": "e1715ff45bede370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_regret(data, best):\n",
    "    return np.abs(best-data)"
   ],
   "id": "c166e0a18a44b483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in model.outcomes:\n",
    "    policy_column = experiments['policy']\n",
    "\n",
    "    # create a DataFrame with all the relevent information\n",
    "    # i.e., policy, scenario_id, and scores\n",
    "    data = pd.DataFrame({outcome.name: outcomes[outcome.name],\n",
    "                         \"policy\":experiments['policy'],\n",
    "                         \"scenario\":experiments['scenario']})\n",
    "\n",
    "    # reorient the data by indexing with policy and scenario id\n",
    "    data = data.pivot(index='scenario', columns='policy')\n",
    "\n",
    "    # flatten the resulting hierarchical index resulting from\n",
    "    # pivoting, (might be a nicer solution possible)\n",
    "    data.columns = data.columns.get_level_values(1)\n",
    "\n",
    "    # we need to control the broadcasting.\n",
    "    # max returns a 1d vector across scenario id. By passing\n",
    "    # np.newaxis we ensure that the shape is the same as the data\n",
    "    # next we take the absolute value\n",
    "    #\n",
    "    # basically we take the difference of the maximum across\n",
    "    # the row and the actual values in the row\n",
    "    #\n",
    "    outcome_regret = (data.max(axis=1).values[:, np.newaxis] - data).abs()\n",
    "\n",
    "    overall_regret[outcome.name] = outcome_regret\n",
    "    max_regret[outcome.name] = outcome_regret.max()"
   ],
   "id": "4f208eb128122ae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_regret = pd.DataFrame(max_regret)\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.savefig(\"regret_single_MORDM.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "64bf4b6f98c94dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "policy_regret = defaultdict(dict)\n",
    "for key, value in overall_regret.items():\n",
    "    for policy in value:\n",
    "        policy_regret[policy][key] = value[policy]"
   ],
   "id": "f2820d32b8e2c979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# this generates a 2 plots with a shared y and x axis\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,5),\n",
    "                         sharey=True, sharex=True)\n",
    "\n",
    "# to ensure easy iteration over the axes grid, we turn it\n",
    "# into a list. Because there are four plots, I hard coded\n",
    "# this.\n",
    "\n",
    "\n",
    "# zip allows us to zip together the list of axes and the list of\n",
    "# key value pairs return by items. If we iterate over this\n",
    "# it returns a tuple of length 2. The first item is the ax\n",
    "# the second items is the key value pair.\n",
    "for ax, (policy, regret) in zip(axes, policy_regret.items()):\n",
    "    data = pd.DataFrame(regret)\n",
    "\n",
    "    # we need to scale the regret to ensure fair visual\n",
    "    # comparison. We can do that by divding by the maximum regret\n",
    "    data = data/max_regret.max(axis=0)\n",
    "    sns.boxplot(data=data, ax=ax)\n",
    "\n",
    "    # removes top and left hand black outline of axes\n",
    "    sns.despine()\n",
    "\n",
    "    # ensure we know which policy the figure is for\n",
    "    ax.set_title(str(policy))\n",
    "plt.show()"
   ],
   "id": "d8b1874bba43e14c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ece041e096880e7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Selecteren scenarios maar gaan we doen met PRIM",
   "id": "e09600d6aed63513"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b72d1a5a3b381931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "99baf9a29e9bdedf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aaf7e99f95e75c61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "policies_df = pd.read_csv(\"../candidate_policies.csv\")\n",
    "\n",
    "# zet elke rij om in een Policy-object\n",
    "candidate_policies = [\n",
    "    Policy(f\"cand_{i}\", **row.to_dict())\n",
    "    for i, row in policies_df.iterrows()\n",
    "\n",
    "    ]"
   ],
   "id": "84dadd451fdb5e5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, _ = get_model_for_problem_formulation(3)\n",
    "    scenarios=100\n",
    "    #policies=4\n",
    "    with MultiprocessingEvaluator(model, n_processes=-1) as evaluator:\n",
    "        results = evaluator.perform_experiments(scenarios=scenarios, policies=candidate_policies )\n",
    "\n",
    "        experiments, outcomes = results"
   ],
   "id": "702460f6dccbd4f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hri_sys = outcomes[\"A.2_HRI per dike\"]\n",
    "hri_q25 = np.percentile(hri_sys, 25)# (N,) systeembreed\n",
    "target_hri = hri_sys <= hri_q25\n",
    "\n",
    "        # ---- 2.2 Expected Annual Damage ----\n",
    "ead_total = outcomes[\"A.2 Total Costs\"] # (N,)\n",
    "ead_q75 = np.percentile(ead_total, 75)\n",
    "target_ead = ead_total >= ead_q75\n",
    "\n",
    "        # # ---- 2.3 RfR Total Costs ----\n",
    "        # rfr_costs = outcomes[\"RfR Total Costs\"].sum(axis=1)  # (N,)\n",
    "        # rfr_q75 = np.percentile(rfr_costs, 75)\n",
    "        # target_rfr = rfr_costs >= rfr_q75\n",
    "\n",
    "        # ---- 2.4 Combineer tot √©√©n worst-case target ----\n",
    "y = target_hri | target_ead        # booleaanse vector (N,)\n",
    "\n",
    "\n",
    "# hri_baseline=baseline_outcomes[\"A.2_HRI per dike\"]\n",
    "# hri_sys = outcomes[\"A.2_HRI per dike\"]\n",
    "# hri_q25 = np.percentile(hri_baseline, 25)# (N,) systeembreed\n",
    "# target_hri = hri_sys <= hri_q25\n",
    "#\n",
    "#         # ---- 2.2 Expected Annual Damage ----\n",
    "# cost_baseline=baseline_outcomes[\"A.2 Total Costs\"]\n",
    "# cost_total = outcomes[\"A.2 Total Costs\"] # (N,)\n",
    "# cost_q75 = np.percentile(cost_baseline, 75)\n",
    "# target_cost = cost_total >= cost_q75\n",
    "\n",
    "        # # ---- 2.3 RfR Total Costs ----\n",
    "        # rfr_costs = outcomes[\"RfR Total Costs\"].sum(axis=1)  # (N,)\n",
    "        # rfr_q75 = np.percentile(rfr_costs, 75)\n",
    "        # target_rfr = rfr_costs >= rfr_q75\n",
    "\n",
    "#         # ---- 2.4 Combineer tot √©√©n worst-case target ----\n",
    "# y = target_hri | target_cost\n",
    "# y\n",
    "\n",
    "print(f\"worst-cases: {y.sum()} van {n_scenarios*policies} runs  ({y.mean()*100:.1f} %)\")\n",
    "\n",
    "        # Determine model uncertainties\n",
    "cols_unc = [u.name for u in model.uncertainties]\n",
    "X = experiments[cols_unc]\n",
    "\n",
    "#Start PRIM\n",
    "prim_alg = prim.Prim(\n",
    "            X, y,\n",
    "            threshold=0.80,\n",
    "        )\n",
    "\n",
    "print(\"Aantal opgeslagen boxen:\", len(prim_alg.boxes))"
   ],
   "id": "8bdd4c1df438398e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# A) De ‚Äúbeste‚Äù box volgens je threshold\n",
    "box = prim_alg.find_box()\n",
    "box.inspect()\n",
    "box.show_tradeoff()\n",
    "plt.show()# geeft een PrimBox terug\n",
    "traj = box.peeling_trajectory\n",
    "print(len(traj), \"stappen in deze box\")"
   ],
   "id": "656b712bc093b5c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# en inspecteer 'm meteen\n",
    "box.inspect( style=\"graph\" )"
   ],
   "id": "ea0b6a9b339baaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = box.show_pairs_scatter()\n",
    "plt.show()"
   ],
   "id": "53e8a05f884b4781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lever_names = [lev.name for lev in model.levers]\n",
    "experiments_unc = experiments.drop(columns=lever_names)\n",
    "\n",
    "\n",
    "# even checken:\n",
    "print(\"origineel aantal kolommen:\", experiments.shape[1])\n",
    "print(\"zonder levers           :\", experiments_unc.shape[1])\n",
    "\n",
    "dimensional_stacking.create_pivot_plot(experiments_unc, y)\n"
   ],
   "id": "50f3f2713f7c860d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 1) Haal de grenzen op uit de box (list met √©√©n entry per box in je peeling_trajectory)\n",
    "raw_limits = box.box_lims[0]\n",
    "\n",
    "# 1) Transponeren, zodat elke var een rij wordt, en de kolommen 0 en 1 zijn\n",
    "df_lim = raw_limits.T\n",
    "\n",
    "# 2) Hernoemen van de kolommen 0‚Üímin en 1‚Üímax\n",
    "df_lim = df_lim.rename(columns={0: \"min\", 1: \"max\"})\n",
    "\n",
    "# 3) (optioneel) zet de index-naam netjes\n",
    "df_lim.index.name = \"var\""
   ],
   "id": "fb01b005ae3a443e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_lim\n",
    "mask = pd.Series(True, index=experiments.index)\n",
    "\n",
    "for var, row in df_lim.iterrows():\n",
    "    lo, hi = row[\"min\"], row[\"max\"]\n",
    "    ser = experiments[var]\n",
    "\n",
    "    if is_numeric_dtype(ser):\n",
    "        # voor numerieke parameters\n",
    "        mask &= ser.between(lo, hi)\n",
    "    else:\n",
    "        # voor categorische parameters\n",
    "        # lo kan een set/list zijn, of een enkele waarde\n",
    "        if isinstance(lo, (set, list)):\n",
    "            mask &= ser.isin(lo)\n",
    "        else:\n",
    "            mask &= (ser == lo)\n",
    "\n",
    "exp_in_box = experiments[mask]\n",
    "out_in_box = {\n",
    "    name: array[mask.values]  # .values is een numpy‚Äêboolean array\n",
    "    for name, array in outcomes.items()\n",
    "}"
   ],
   "id": "4d7e57d7f7e065a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "expanded = {}\n",
    "for name, arr in out_in_box.items():\n",
    "    if arr.ndim == 1:\n",
    "        # gewoon een vector\n",
    "        expanded[name] = arr\n",
    "    else:\n",
    "        # maak per tijdstap een kolom\n",
    "        T = arr.shape[1]\n",
    "        for t in range(T):\n",
    "            expanded[f\"{name}_{t}\"] = arr[:, t]\n",
    "\n",
    "out_df = pd.DataFrame(expanded)"
   ],
   "id": "f5f8586dc63b43f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "exp_in_box['worst_case']=y[exp_in_box.index]\n",
    "true_worst= exp_in_box[exp_in_box['worst_case']]\n",
    "#\n",
    "# true_worst = true_worst.assign(\n",
    "#     total_cost = out_in_box['A.2 Total Costs']\n",
    "# )"
   ],
   "id": "3d51f6cdcec44fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 1) Maak √©√©n Series van alle total costs, met de originele index ---\n",
    "all_costs = pd.Series(\n",
    "    out_in_box['A.2 Total Costs'],       # numpy array voor _alle_ runs in out_in_box\n",
    "    index = exp_in_box.index              # die hoort dezelfde lengte en index labels te hebben\n",
    ")\n",
    "\n",
    "# --- 2) Filter die Series naar alleen de worst‚Äêcase subset (true_worst.index) ---\n",
    "worst_costs = all_costs.loc[true_worst.index]\n",
    "\n",
    "# --- 3) Wijs die gefilterde Series toe als nieuwe kolom ---\n",
    "true_worst = true_worst.assign(total_costs=worst_costs)"
   ],
   "id": "d986764991ce40c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "true_worst['total_costs'] = pd.Series(\n",
    "    out_in_box['A.2 Total Costs'],\n",
    "    index=exp_in_box.index\n",
    ").loc[true_worst.index]"
   ],
   "id": "94e40122ff8fda00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "true_worst",
   "id": "7f2013413625662e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 1) Maak √©√©n Series van alle total costs, met de originele index ---\n",
    "all_costs = pd.Series(\n",
    "    out_in_box['A.2_HRI per dike'],       # numpy array voor _alle_ runs in out_in_box\n",
    "    index = exp_in_box.index              # die hoort dezelfde lengte en index labels te hebben\n",
    ")\n",
    "\n",
    "# --- 2) Filter die Series naar alleen de worst‚Äêcase subset (true_worst.index) ---\n",
    "worst_HRI = all_costs.loc[true_worst.index]\n",
    "\n",
    "# --- 3) Wijs die gefilterde Series toe als nieuwe kolom ---\n",
    "true_worst = true_worst.assign(HRI=worst_HRI)"
   ],
   "id": "14e6ac694ca82437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "true_worst",
   "id": "91c93fdf2bb55eed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) min-max schaal HRI en costs\n",
    "hri = true_worst['HRI']\n",
    "cost = true_worst['total_costs']\n",
    "\n",
    "hri_scaled = (hri - hri.min()) / (hri.max() - hri.min())\n",
    "cost_scaled = (cost - cost.min()) / (cost.max() - cost.min())\n",
    "\n",
    "# 2) keer de costs om\n",
    "inv_cost = 1 - cost_scaled\n",
    "\n",
    "# 3) bereken de samengestelde score\n",
    "score = hri_scaled * inv_cost\n",
    "\n",
    "# 4) voeg toe aan true_worst\n",
    "true_worst = true_worst.assign(\n",
    "    hri_scaled = hri_scaled,\n",
    "    cost_scaled = cost_scaled,\n",
    "    inv_cost    = inv_cost,\n",
    "    score       = score\n",
    ")\n",
    "\n",
    "true_worst.head()"
   ],
   "id": "32d2d346a2d946c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # 1) voeg y als kolom toe\n",
    "# exp_in_box = exp_in_box.copy()\n",
    "# exp_in_box['worst_case'] = y[exp_in_box.index]\n",
    "#\n",
    "# # 2) houd alleen echte worst-cases over\n",
    "# true_worst = exp_in_box[exp_in_box['worst_case']]\n",
    "\n",
    "# 3) kies uit true_worst je max/min/median voor EAD & HRI\n",
    "best  = true_worst.nlargest(1, 'score')   #best case is hoogste\n",
    "worst = true_worst.nsmallest(1,  'score')      #worst case is laagste\n",
    "mid   = true_worst.iloc[[len(true_worst)//2]]"
   ],
   "id": "978d568e565589ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) knijp de rij samen naar een Series\n",
    "best_s   = best.iloc[0]  # of .iloc[0]\n",
    "mid_s = mid.iloc[0]\n",
    "worst_s  = worst.iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "df_scenarios = pd.DataFrame([\n",
    "    best_s.to_dict(),\n",
    "    mid_s.to_dict(),\n",
    "    worst_s.to_dict()\n",
    "], index=[\"best_case\", \"median_case\", \"worst_case\"])\n",
    "\n",
    "df_scenarios = (\n",
    "    df_scenarios\n",
    "    .rename(columns={\"scenario.1\": \"scenario\"})  # als hij zo heet\n",
    "    .set_index(\"scenario\")                        # zet de kolom als index\n",
    ")"
   ],
   "id": "4b249e5328acd04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_scenarios",
   "id": "7c26262167e1378e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_scenarios.to_csv(\"reference_scenarios.csv\", index_label=\"scenario\")\n",
   "id": "a98544c94f927716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "candidate_scenarios = [\n",
    "    Scenario(f\"scenario_{idx}\", **row.to_dict())\n",
    "    for idx, row in df_scenarios.iterrows()\n",
    "]"
   ],
   "id": "4c846aa3bcb9370e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for scenario in candidate_scenarios:\n",
    "    print(scenario)"
   ],
   "id": "4cf0aa4701208743"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## scenario's uit PRIM",
   "id": "ac44343e73930149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_scenarios= pd.read_csv('reference_scenarios.csv')\n",
    "#\n",
    "# df_scenarios = (\n",
    "#     df_scenarios\n",
    "#     .rename(columns={\"scenario.1\": \"scenario\"})\n",
    "#     .set_index(\"scenario\")            )"
   ],
   "id": "95aee1c1c91a06ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_scenarios=df_scenarios[cols_unc]\n",
    "# df_scenarios"
   ],
   "id": "e63103a7a4137ae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # candidate_scenarios = [\n",
    "    #     Scenario(f\"scenario_{idx}\", **row.to_dict())\n",
    "    #     for idx, row in df_scenarios.iterrows()\n",
    "    # ]"
   ],
   "id": "f4df3ff7f1f4645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## vanaf hier wordt het multi MORDM",
   "id": "18f41d8612cee78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n"
   ],
   "id": "ac77478147a5907f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench import MultiprocessingEvaluator, ema_logging\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress\n",
    "from ema_workbench.em_framework.optimization import to_problem\n",
    "import pandas as pd\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "\n",
    "nfe = 100\n",
    "epsilons = [0.01] * len(model.outcomes)\n",
    "\n",
    "results = []\n",
    "convergence_records = []\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    for i, scenario in enumerate(candidate_scenarios[:3]):\n",
    "        #for seed in range(3):\n",
    "        print(f\"üîÑ Running: Scenario {i} ({scenario.name}) \")\n",
    "\n",
    "        eps_prog = EpsilonProgress()\n",
    "        result, convergence = evaluator.optimize(\n",
    "                nfe=nfe,\n",
    "                searchover=\"levers\",\n",
    "                epsilons=epsilons,\n",
    "                convergence=[eps_prog],\n",
    "                reference=scenario,\n",
    "                constraints = model.constraints,# ‚Üê hier toevoegen\n",
    "\n",
    "            )\n",
    "\n",
    "            # Store optimization results\n",
    "        results.append(pd.DataFrame(result))\n",
    "\n",
    "            # The convergence object is a DataFrame already\n",
    "        convergence_df = pd.DataFrame(convergence)\n",
    "        convergence_df[\"scenario\"] = scenario.name\n",
    "        #convergence_df[\"seed\"] = seed\n",
    "        convergence_records.append(convergence_df)\n",
    "\n",
    "# Merge final dataframes\n",
    "combined_results = pd.concat(results, ignore_index=True)\n",
    "convergence_df = pd.concat(convergence_records, ignore_index=True)"
   ],
   "id": "d992a9b4809f3896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # === PLOT Œµ-PROGRESS BY SCENARIO ===\n",
    "# import matplotlib as mpl\n",
    "#\n",
    "# fig, ax = plt.subplots(figsize=(8, 4))\n",
    "# colors = sns.color_palette()\n",
    "# legend_items = []\n",
    "#\n",
    "# for (scenario_name, scores), color in zip(convergence_df.groupby(\"scenario\"), colors):\n",
    "#     # Create custom legend item\n",
    "#     legend_items.append((mpl.lines.Line2D([0, 0], [1, 1], c=color), scenario_name))\n",
    "#\n",
    "#     # Plot each seed\n",
    "#     #for seed, score in scores.groupby(\"seed\"):\n",
    "#     ax.plot(score.nfe, score.epsilon_progress, c=color, lw=1)\n",
    "#\n",
    "# ax.set_ylabel(r'$\\epsilon$ progress')\n",
    "# ax.set_xlabel('number of function evaluations')\n",
    "#\n",
    "# # Add legend\n",
    "# artists, labels = zip(*legend_items)\n",
    "# fig.legend(artists, labels, bbox_to_anchor=(1.02, 0.9))\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"convergence_multi_mordm.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ],
   "id": "5ed88412eb011999"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f3bfadc7e8eb1d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "combined_results",
   "id": "b2cf575c4218ffff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(combined_results)",
   "id": "832da3e701c63988"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = combined_results.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "    # Set larger figure size (wider and taller)\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.savefig(\"multi mordm all.png\", dpi=300, bbox_inches='tight') # width=12 inches, height=8 inches\n"
   ],
   "id": "1ae35feb2f27efe1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Generate parallel axes with your limits\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "# Plot the data\n",
    "paraxes.plot(data)\n",
    "\n",
    "# Invert specific axis\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"mordm_parallel_coordinates.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "e5dee6c4f91d1e5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a6faf15701e9ac08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "logical = (\n",
    "    (combined_results[\"A.2 Total Costs\"] < 7e7) &\n",
    "\n",
    "    (combined_results[\"A.2_HRI per dike\"] > 1))\n",
    "\n",
    "\n",
    "\n",
    "np.sum(logical)\n",
    "combined_results[logical]"
   ],
   "id": "d9bcd392bab7b5dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "results_2 = combined_results[logical]\n",
    "results_2[\"policy\"] = results_2.index  # Automatically uses 16, 17, 18 in your case\n",
    "\n",
    "\n",
    "data = results_2.loc[:, [o.name for o in model.outcomes] + ['policy']]\n",
    "\n",
    "\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['A.2 Total Costs', 'A.2_Expected Number of Deaths',\n",
    "               'RfR Total Costs', 'Expected Evacuation Costs']] = 0\n",
    "\n",
    "\n",
    "policy_ids = data[\"policy\"].unique()\n",
    "colors = sns.color_palette(\"tab10\", len(policy_ids))\n",
    "color_map = dict(zip(policy_ids, colors))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "\n",
    "# Plot each policy row with its assigned color\n",
    "for _, row in data.iterrows():\n",
    "    policy_id = row[\"policy\"]\n",
    "    color = color_map.get(policy_id, \"gray\")\n",
    "    paraxes.plot(row.to_frame().T, color=color, alpha=0.8)\n",
    "\n",
    "# Invert axis if needed\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "\n",
    "\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], color=color_map[pid], label=f\"Policy {pid}\")\n",
    "    for pid in policy_ids\n",
    "]\n",
    "plt.legend(handles=legend_handles, title=\"Policy ID\", loc=\"center left\",bbox_to_anchor=(1.02, 0.5),borderaxespad=0)\n",
    "\n",
    "\n",
    "plt.savefig(\"parallel_coords_multi_MORDM_selected_policies.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "56acc98145de6a0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "policies = combined_results[logical]\n",
    "policies = policies.drop([o.name for o in model.outcomes], axis=1)\n",
    "policies"
   ],
   "id": "36536afecbe55f30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test=combined_results[logical]\n",
    "test"
   ],
   "id": "cb0cd3e69fcf4f02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hier onder worden de policies gemaakt die getest moeten worden",
   "id": "f10665b3e4f0e50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))"
   ],
   "id": "1208d64a2eb96cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_scenarios = 100\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)"
   ],
   "id": "ec8efa14a61058a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results",
   "id": "1914a8e7b1ec8294"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    if std==0:\n",
    "        std = 1\n",
    "\n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std\n"
   ],
   "id": "40a250875b399d1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ema_workbench import ScalarOutcome\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "\n",
    "    logical = experiments['policy']==policy\n",
    "\n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ],
   "id": "b4c269e754187c82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis(\"RfR Total Costs\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "4035ba41a0428bc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "158e2e0430086d1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in model.outcomes:\n",
    "    policy_column = experiments['policy']\n",
    "\n",
    "    # create a DataFrame with all the relevent information\n",
    "    # i.e., policy, scenario_id, and scores\n",
    "    data = pd.DataFrame({outcome.name: outcomes[outcome.name],\n",
    "                         \"policy\":experiments['policy'],\n",
    "                         \"scenario\":experiments['scenario']})\n",
    "\n",
    "    # reorient the data by indexing with policy and scenario id\n",
    "    data = data.pivot(index='scenario', columns='policy')\n",
    "\n",
    "    # flatten the resulting hierarchical index resulting from\n",
    "    # pivoting, (might be a nicer solution possible)\n",
    "    data.columns = data.columns.get_level_values(1)\n",
    "\n",
    "    # we need to control the broadcasting.\n",
    "    # max returns a 1d vector across scenario id. By passing\n",
    "    # np.newaxis we ensure that the shape is the same as the data\n",
    "    # next we take the absolute value\n",
    "    #\n",
    "    # basically we take the difference of the maximum across\n",
    "    # the row and the actual values in the row\n",
    "    #\n",
    "    outcome_regret = (data.max(axis=1).values[:, np.newaxis] - data).abs()\n",
    "\n",
    "    overall_regret[outcome.name] = outcome_regret\n",
    "    max_regret[outcome.name] = outcome_regret.max()"
   ],
   "id": "2eb2335da2c673a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_regret = pd.DataFrame(max_regret)\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.savefig(\"regret multi mordm.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "88fd44874ff1f862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiments",
   "id": "a0a9602395eef7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "policies",
   "id": "88f3309e5ba8e4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "policies.to_csv(\"multi-scenario_policies.csv\", index=True)\n",
   "id": "269a4ae9aa8cd00f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PRIM analyse met multi scenario policies voor robustness",
   "id": "ec9e5bc6a64a9794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# lees de CSV in met de gevonden policies vanuit de MORDM\n",
    "policies_multiscenario = pd.read_csv(\"../multi-scenario_policies.csv\")\n",
    "\n",
    "# zet elke rij om in een Policy-object\n",
    "multiscenario_policies = [\n",
    "    Policy(f\"cand_{i}\", **row.to_dict())\n",
    "    for i, row in policies_multiscenario.iterrows()\n",
    "\n",
    "    ]\n"
   ],
   "id": "43e376b743bf561c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, _ = get_model_for_problem_formulation(3)\n",
    "\n",
    "    df_pols = pd.read_csv(\"../multi-scenario_policies.csv\")\n",
    "\n",
    "    # policies = [\n",
    "    #     Policy(f\"policy_{i}\", **row.to_dict())\n",
    "    #     for i, row in df_pols.iterrows()\n",
    "    # ]\n",
    "\n",
    "    scenarios=100\n",
    "    policies=multiscenario_policies #policies retrieved from multi scenario MORDM\n",
    "    with MultiprocessingEvaluator(model, n_processes=-1) as evaluator:\n",
    "        results = evaluator.perform_experiments(scenarios=scenarios, policies=policies )\n",
    "\n",
    "        experiments, outcomes = results"
   ],
   "id": "c8887ce6196ce880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiments",
   "id": "f062ed5560875c5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b84395d4494dba8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_per_policy = {}\n",
    "\n",
    "hri_baseline=baseline_outcomes[\"A.2_HRI per dike\"]\n",
    "hri_sys = outcomes[\"A.2_HRI per dike\"]\n",
    "hri_q25 = np.percentile(hri_baseline, 25)# (N,) systeembreed\n",
    "target_hri = hri_sys <= hri_q25\n",
    "\n",
    "        # ---- 2.2 Expected Annual Damage ----\n",
    "cost_baseline=baseline_outcomes[\"A.2 Total Costs\"]\n",
    "cost_total = outcomes[\"A.2 Total Costs\"] # (N,)\n",
    "cost_q75 = np.percentile(cost_baseline, 75)\n",
    "target_cost = cost_total >= cost_q75\n",
    "\n",
    "        # # ---- 2.3 RfR Total Costs ----\n",
    "        # rfr_costs = outcomes[\"RfR Total Costs\"].sum(axis=1)  # (N,)\n",
    "        # rfr_q75 = np.percentile(rfr_costs, 75)\n",
    "        # target_rfr = rfr_costs >= rfr_q75\n",
    "\n",
    "        # ---- 2.4 Combineer tot √©√©n worst-case target ----\n",
    "y = target_hri | target_cost\n",
    "y\n"
   ],
   "id": "2d1b4d50c531fbb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7433d671d99871b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "c23c6ddc2c1a082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "9159c730c61faae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for pol in multiscenario_policies:\n",
    "\n",
    "    pol_name = pol.name\n",
    "\n",
    "    # selecteer alleen de runs met exact die naam\n",
    "    mask_pol = experiments[\"policy\"] == pol_name\n",
    "    X_pol   = experiments.loc[mask_pol, [u.name for u in model.uncertainties]]\n",
    "    y_pol   = y[mask_pol]\n",
    "\n",
    "    # 3) sanity‚Äêchecks\n",
    "    # 3) sanity‚Äêchecks\n",
    "    print(f\"Policy {pol.name!r}: {len(X_pol)} runs (y=True: {y_pol.sum()}, y=False: {len(y_pol)-y_pol.sum()})\")\n",
    "\n",
    "    if len(X_pol) == 0:\n",
    "        print(\"  ‚Üí geen runs met deze policy, overslaan\")\n",
    "        continue\n",
    "\n",
    "    if not (y_pol.dtype == bool):\n",
    "        y_pol = y_pol.astype(bool)\n",
    "\n",
    "    if y_pol.sum() == 0 or y_pol.sum() == len(y_pol):\n",
    "        print(\"  ‚Üí y bevat niet zowel True als False, overslaan\")\n",
    "        continue\n",
    "\n",
    "    # 2) maak en run PRIM\n",
    "    prim_alg = prim.Prim(\n",
    "        X_pol,\n",
    "        y_pol,\n",
    "        threshold=0.6,       # kies zelf je cover‚Äêof‚Äêdensity cut‚Äêoff\n",
    "        peel_alpha=0.05,     # hoe ‚Äúagressief‚Äù peel je\n",
    "        mass_min=0.1        # minimale fractie runs in box\n",
    "    )\n",
    "    box = prim_alg.find_box()\n",
    "\n",
    "    box.inspect()\n",
    "    #box.show_tradeoff()\n",
    "\n",
    "    traj = box.peeling_trajectory\n",
    "\n",
    "\n",
    "    # 3) bewaar je result voor latere vergelijking\n",
    "    results_per_policy[pol_name] = {\n",
    "        \"prim_alg\":   prim_alg,\n",
    "        \"selected_box\": box,\n",
    "        \"trajectory\":   traj,\n",
    "    }\n",
    "        # \"box_id\": int(traj.score.idxmax())"
   ],
   "id": "b5509d04b8715a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) lijst van alle uncertainty-kolommen\n",
    "cols_unc = [u.name for u in model.uncertainties]\n",
    "\n",
    "# 2) per kolom zelf min/max bepalen, alleen als 'ie numeriek is\n",
    "global_ranges = {}\n",
    "for var in cols_unc:\n",
    "    ser = experiments[var]\n",
    "    if is_numeric_dtype(ser):\n",
    "        lo = ser.min()\n",
    "        hi = ser.max()\n",
    "        global_ranges[var] = (lo, hi)\n",
    "    else:\n",
    "        # categoricals slaan we over, of bewaar je volledige domain als set:\n",
    "        cats = ser.cat.categories if hasattr(ser.dtype, \"categories\") else None\n",
    "        print(f\"  ‚Üí sla {var!r} over (dtype={ser.dtype})\")\n",
    "\n",
    "# 3) print ter controle\n",
    "for var, (lo, hi) in global_ranges.items():\n",
    "    print(f\"{var:30s}  glob_min = {lo:8.3f},  glob_max = {hi:8.3f}\")"
   ],
   "id": "8eec296d35ced433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "overall = (\n",
    "    experiments\n",
    "      .groupby(\"policy\")\n",
    "      .apply(lambda df: y[df.index].mean())\n",
    "      .rename(\"overall_fail_rate\")\n",
    ")\n",
    "\n",
    "# --- 2) verzamel PRIM‚Äêmetrics uit de geselecteerde box voor elke policy ---\n",
    "box_info = []\n",
    "for pol, info in results_per_policy.items():\n",
    "    box = info[\"selected_box\"]   # dit is een PrimBox\n",
    "    box_info.append({\n",
    "        \"policy\":   pol,\n",
    "        \"coverage\": box.coverage,\n",
    "        \"density\":  box.density,\n",
    "        \"res_dim\":  box.res_dim,\n",
    "        \"mass\":     box.mass\n",
    "    })\n",
    "\n",
    "box_df = pd.DataFrame(box_info).set_index(\"policy\")\n",
    "\n",
    "# --- 3) combineer alles in √©√©n rapport-DataFrame ---\n",
    "report = pd.concat([overall, box_df], axis=1)\n",
    "\n",
    "# zet de fracties om in percentages\n",
    "report[\"overall_fail_%\"]       = report.overall_fail_rate * 100\n",
    "report[\"box_covers_%\"]         = report.coverage        * 100\n",
    "report[\"fail_in_box_%\"]        = report.density         * 100\n",
    "\n",
    "# selecteer en sorteer kolommen\n",
    "report = report[[\n",
    "    \"overall_fail_%\",\n",
    "    \"box_covers_%\",\n",
    "    \"fail_in_box_%\",\n",
    "    \"res_dim\",\n",
    "    \"mass\"\n",
    "]].sort_values(\"overall_fail_%\", ascending=False)\n",
    "\n",
    "# afronden op 1 decimaal\n",
    "report = report.round(1)\n",
    "\n",
    "report"
   ],
   "id": "da5105dd4c237e82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test= experiments['policy']=='cand_23'",
   "id": "36188ad66f153b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# names of all levers in the model\n",
    "lever_names = [lev.name for lev in model.levers]\n",
    "\n",
    "# pick the policy you want to inspect\n",
    "pol_name = \"cand_0\"          # or any other policy name\n",
    "\n",
    "# show the lever settings for that policy (one row per scenario)\n",
    "experiments.loc[experiments[\"policy\"] == pol_name, lever_names]\n"
   ],
   "id": "faa0a168fd8b4e26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b6c8f23a88eff577"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25ce61768eb568ac"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
