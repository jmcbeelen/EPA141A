{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1a9a5103725429e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import functools\n",
    "from ema_workbench import Scenario\n",
    "from ema_workbench import Model, MultiprocessingEvaluator, Policy, Scenario\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import perform_experiments\n",
    "from ema_workbench.em_framework.samplers import sample_uncertainties\n",
    "from ema_workbench.util import ema_logging\n",
    "import time\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench.analysis import pairs_plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ema_workbench.analysis import prim\n",
    "from ema_workbench.analysis import dimensional_stacking\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "from sklearn import preprocessing\n",
    "from ema_workbench.em_framework.optimization import (ArchiveLogger,\n",
    "                                                     EpsilonProgress,\n",
    "                                                     to_problem, epsilon_nondominated)\n",
    "\n",
    "from ema_workbench import ScalarOutcome"
   ],
   "id": "5bae2cfa960d75dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # generic function for calculating domain criterion robustness metric\n",
    "# def robustness(direction, threshold, data):\n",
    "#     if direction == SMALLER:\n",
    "#         return np.sum(data<=threshold)/data.shape[0]\n",
    "#     else:\n",
    "#         return np.sum(data>=threshold)/data.shape[0]\n",
    "#\n",
    "# SMALLER = 'SMALLER'\n",
    "# LARGER = 'LARGER'\n",
    "#\n",
    "# # we use a functional programming trick with the generic robustness\n",
    "# # function to create our actual robustness function for each\n",
    "# # objective (see https://docs.python.org/3/library/functools.html#functools.partial)\n",
    "# maxp = functools.partial(robustness, SMALLER, 0.75)\n",
    "# inertia = functools.partial(robustness, LARGER, 0.6)"
   ],
   "id": "becfe217cfd4020a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_scenarios= pd.read_csv('reference_scenarios.csv')\n",
    "\n",
    "df_scenarios = (\n",
    "    df_scenarios\n",
    "    .rename(columns={\"scenario.1\": \"scenario\"})  # als hij zo heet\n",
    "    .set_index(\"scenario\")                        # zet de kolom als index\n",
    ")\n",
    "\n",
    "candidate_scenarios = [\n",
    "    Scenario(f\"scenario_{idx}\", **row.to_dict())\n",
    "    for idx, row in df_scenarios.iterrows()\n",
    "]"
   ],
   "id": "789f23e9714a6de2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# lees de CSV in met de gevonden policies vanuit de MORDM\n",
    "policies_df = pd.read_csv(\"candidate_policies.csv\")\n",
    "\n",
    "# zet elke rij om in een Policy-object\n",
    "candidate_policies = [\n",
    "    Policy(f\"cand_{i}\", **row.to_dict())\n",
    "    for i, row in policies_df.iterrows()\n",
    "\n",
    "    ]"
   ],
   "id": "2cce2a8601c403d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, _ = get_model_for_problem_formulation(3)\n",
    "    scenarios=10\n",
    "    policies=4\n",
    "    with MultiprocessingEvaluator(model, n_processes=-1) as evaluator:\n",
    "        results = evaluator.perform_experiments(scenarios=scenarios, policies=candidate_policies )\n",
    "\n",
    "        experiments, outcomes = results"
   ],
   "id": "134f0e8d7384a6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "convergence_metrics = [EpsilonProgress()]\n",
    "\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.robust_optimize(\n",
    "        nfe=0,                            # no new search, just evaluate your eight policies\n",
    "        searchover=\"policies\",           # search space is your list of policies\n",
    "        policies=candidate_policies,\n",
    "        reference=candidate_scenarios,   # multi‐scenario evaluation\n",
    "        convergence=[EpsilonProgress()], # track progress (optional)\n",
    "    )\n",
    "    results, convergence = evaluator.robust_optimize(robustness_functions, candidate_scenarios,\n",
    "                                                     nfe=nfe, epsilons=[0.05,]*len(robustness_functions),\n",
    "                                                     convergence=convergence_metrics,)\n",
    "\n",
    "# 4) look at your results\n",
    "#    - one row per policy\n",
    "#    - columns give you robust performance metrics\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# e.g. sort by 5th‐percentile of your worst‐case outcome (or whichever metric you care about)\n",
    "df = df.sort_values(by=\"outcome_name__5%\", ascending=False)\n",
    "\n",
    "print(df.head())"
   ],
   "id": "18fd561e39acdbeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e16f4b22b02de95"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
